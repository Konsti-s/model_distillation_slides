## Modern Applications

Distillation is Everywhere

**Production Models:**
* **DistilBERT**: 40% smaller, 60% faster, 97% performance
* **OpenAI**: GPT → 4o-mini / o1-mini / o3-mini 
* **Google**: Gemini  → Nano / Flash
* **Anthropic**: Claude -> Haiku / Sonnet
* **Deepseek**: Distilled from GPT-Models (overwhelming technical evidence)

> A huge share of the models you interact with is a distilled model - smaller, faster, cheaper, but trained on the knowledge of giants