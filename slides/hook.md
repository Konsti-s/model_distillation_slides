## Model Distillation

### "How to reduce model size of LLMs?"

<img src="images/distillation.png" style="width: 30%; height: auto; margin: 20px auto; display: block;">

**Direct comparison:**
* GPT-4o: ~200 billion parameters
* GPT-4o-mini: ~8 billion parameters

> Condensing knowledge of a network into a considerably smaller network with little performance sacrifice.